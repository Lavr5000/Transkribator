---
phase: 01-critical-bug-fixes
plan: 04
type: execute
wave: 2
depends_on: ["01-01", "01-02"]
files_modified:
  - tests/test_backend_quality.py  # NEW FILE
  - tests/fixtures/audio_samples/  # NEW DIRECTORY
autonomous: true

must_haves:
  truths:
    - "Фреймворк для тестирования качества создан"
    - "WER (Word Error Rate) метрика измеряется"
    - "CER (Character Error Rate) метрика измеряется"
    - "RTF (Real-Time Factor) метрика измеряется"
  artifacts:
    - path: "tests/test_backend_quality.py"
      min_lines: 100
      contains: "def calculate_wer"
      contains: "def calculate_cer"
      contains: "def measure_rtf"
    - path: "tests/fixtures/audio_samples/"
      contains: "README.md"
  key_links:
    - from: "tests/test_backend_quality.py"
      to: "src/backends/whisper_backend.py"
      via: "import in test"
      pattern: "from.*backends.*import.*WhisperBackend"
---

<objective>
Create testing framework for measuring transcription quality.

**What:**
Build test framework with WER, CER, and RTF metrics to validate improvements.

**Why:**
Can't verify "15-30% improvement" without measuring. Need baseline metrics before and after changes to validate Phase 1 success.

**Output:**
test_backend_quality.py with metrics calculation and sample audio fixtures.
</objective>

<execution_context>
@C:\Users\user\.claude\get-shit-down\workflows/execute-plan.md
@C:\Users\user\.claude\get-shit-down\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/STATE.md

# Test requirements (from REQUIREMENTS.md):
# TEST-01: A/B тестирование до/после улучшений
# TEST-02: Измерение WER (Word Error Rate)
# TEST-03: Измерение CER (Character Error Rate)
# TEST-04: Сравнение с WhisperTyping
# TEST-05: Измерение RTF (Real-Time Factor)

# Project structure:
# C:\Users\user\.claude\0 ProEKTi\Transkribator\
#   src/          # Main source
#   tests/        # Test directory (create if not exists)
#   fixtures/     # Test audio samples
</context>

<tasks>

<task type="auto">
  <name>Create quality metrics test framework</name>
  <files>tests/test_backend_quality.py, tests/__init__.py</files>
  <action>
Create testing framework at tests/test_backend_quality.py:

```python
"""Test framework for measuring ASR quality metrics.

Implements:
- WER (Word Error Rate): (substitutions + deletions + insertions) / total_words
- CER (Character Error Rate): Same as WER but for characters
- RTF (Real-Time Factor): processing_time / audio_duration
"""
import os
import time
from pathlib import Path
from typing import Tuple, List
import numpy as np

import sys
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from backends.whisper_backend import WhisperBackend
from backends.sherpa_backend import SherpaBackend


def calculate_wer(reference: str, hypothesis: str) -> float:
    """Calculate Word Error Rate.

    WER = (S + D + I) / N
    S = substitutions, D = deletions, I = insertions, N = total words

    Args:
        reference: Ground truth text
        hypothesis: Transcribed text

    Returns:
        WER as float between 0.0 and 1.0 (lower is better)
    """
    ref_words = reference.lower().split()
    hyp_words = hypothesis.lower().split()

    # Simple Levenshtein distance for words
    m, n = len(ref_words), len(hyp_words)
    dp = [[0] * (n + 1) for _ in range(m + 1)]

    for i in range(m + 1):
        dp[i][0] = i
    for j in range(n + 1):
        dp[0][j] = j

    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if ref_words[i - 1] == hyp_words[j - 1]:
                dp[i][j] = dp[i - 1][j - 1]
            else:
                dp[i][j] = 1 + min(
                    dp[i - 1][j],    # deletion
                    dp[i][j - 1],    # insertion
                    dp[i - 1][j - 1]  # substitution
                )

    total_errors = dp[m][n]
    total_words = max(len(ref_words), 1)  # Avoid division by zero
    return total_errors / total_words


def calculate_cer(reference: str, hypothesis: str) -> float:
    """Calculate Character Error Rate.

    Similar to WER but operates on characters instead of words.
    Better for morphological languages like Russian.

    Args:
        reference: Ground truth text
        hypothesis: Transcribed text

    Returns:
        CER as float between 0.0 and 1.0 (lower is better)
    """
    ref_chars = list(reference.lower().replace(" ", ""))
    hyp_chars = list(hypothesis.lower().replace(" ", ""))

    m, n = len(ref_chars), len(hyp_chars)
    dp = [[0] * (n + 1) for _ in range(m + 1)]

    for i in range(m + 1):
        dp[i][0] = i
    for j in range(n + 1):
        dp[0][j] = j

    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if ref_chars[i - 1] == hyp_chars[j - 1]:
                dp[i][j] = dp[i - 1][j - 1]
            else:
                dp[i][j] = 1 + min(
                    dp[i - 1][j],
                    dp[i][j - 1],
                    dp[i - 1][j - 1]
                )

    total_errors = dp[m][n]
    total_chars = max(len(ref_chars), 1)
    return total_errors / total_chars


def measure_rtf(audio_duration: float, processing_time: float) -> float:
    """Calculate Real-Time Factor.

    RTF = processing_time / audio_duration
    RTF < 1.0 means faster than real-time

    Args:
        audio_duration: Length of audio in seconds
        processing_time: Time taken to transcribe in seconds

    Returns:
        RTF as float (lower is better)
    """
    if audio_duration == 0:
        return float('inf')
    return processing_time / audio_duration


class QualityTestRunner:
    """Run quality tests on backends."""

    def __init__(self, fixtures_dir: Path = None):
        self.fixtures_dir = fixtures_dir or Path(__file__).parent / "fixtures" / "audio_samples"
        self.results = []

    def test_backend(
        self,
        backend,
        audio_path: Path,
        reference_text: str,
        backend_name: str
    ) -> dict:
        """Test a single backend with audio sample.

        Args:
            backend: Backend instance (WhisperBackend or SherpaBackend)
            audio_path: Path to audio file
            reference_text: Ground truth transcription
            backend_name: Name for reporting

        Returns:
            Dict with wer, cer, rtf, transcribed_text
        """
        try:
            # Load audio
            try:
                import librosa
                audio, sr = librosa.load(str(audio_path), sr=16000)
                audio_duration = len(audio) / sr
            except ImportError:
                from scipy.io import wavfile
                sr, audio = wavfile.read(str(audio_path))
                audio = audio.astype(np.float32) / 32768.0
                audio_duration = len(audio) / sr

            # Transcribe
            transcribed_text, processing_time = backend.transcribe(audio, sr)

            # Calculate metrics
            wer = calculate_wer(reference_text, transcribed_text)
            cer = calculate_cer(reference_text, transcribed_text)
            rtf = measure_rtf(audio_duration, processing_time)

            result = {
                "backend": backend_name,
                "audio_file": audio_path.name,
                "reference": reference_text,
                "transcribed": transcribed_text,
                "wer": wer,
                "cer": cer,
                "rtf": rtf,
                "processing_time": processing_time,
                "audio_duration": audio_duration,
            }
            self.results.append(result)
            return result

        except Exception as e:
            return {
                "backend": backend_name,
                "audio_file": audio_path.name,
                "error": str(e),
                "wer": 1.0,  # Max error
                "cer": 1.0,
                "rtf": float('inf'),
            }

    def print_report(self):
        """Print summary report of all tests."""
        print("\n" + "=" * 70)
        print("ASR QUALITY TEST REPORT")
        print("=" * 70)

        for r in self.results:
            if "error" in r:
                print(f"\n{r['backend']} - {r['audio_file']}: ERROR - {r['error']}")
                continue

            print(f"\n{r['backend']} - {r['audio_file']}:")
            print(f"  Reference:    {r['reference'][:60]}...")
            print(f"  Transcribed:  {r['transcribed'][:60]}...")
            print(f"  WER:          {r['wer']:.4f} ({r['wer']*100:.1f}%)")
            print(f"  CER:          {r['cer']:.4f} ({r['cer']*100:.1f}%)")
            print(f"  RTF:          {r['rtf']:.3f} ({'faster' if r['rtf'] < 1 else 'slower'} than real-time)")
            print(f"  Processed in: {r['processing_time']:.2f}s (audio: {r['audio_duration']:.2f}s)")

        print("\n" + "=" * 70)


def main():
    """Run example quality test."""
    print("ASR Quality Test Framework")
    print("=" * 50)

    # Create test fixtures directory if not exists
    fixtures_dir = Path(__file__).parent / "fixtures" / "audio_samples"
    fixtures_dir.mkdir(parents=True, exist_ok=True)

    # Check for test audio
    test_files = list(fixtures_dir.glob("*.wav")) + list(fixtures_dir.glob("*.mp3"))
    if not test_files:
        print(f"\nNo test audio found in {fixtures_dir}")
        print("Add audio samples with corresponding .txt reference files")
        print("Example: test.wav + test.wav.txt (contains reference text)")
        return

    # Initialize backends
    try:
        whisper = WhisperBackend(model_size="base", language="ru")
        print("✓ Whisper backend initialized")
    except Exception as e:
        print(f"✗ Whisper backend failed: {e}")
        whisper = None

    try:
        sherpa = SherpaBackend(model_size="giga-am-v2-ru", language="ru")
        print("✓ Sherpa backend initialized")
    except Exception as e:
        print(f"✗ Sherpa backend failed: {e}")
        sherpa = None

    # Run tests
    runner = QualityTestRunner(fixtures_dir)

    for audio_file in test_files:
        ref_file = fixtures_dir / f"{audio_file.name}.txt"
        if not ref_file.exists():
            print(f"Skipping {audio_file.name} (no reference text)")
            continue

        reference_text = ref_file.read_text(encoding="utf-8").strip()

        if whisper:
            runner.test_backend(whisper, audio_file, reference_text, "Whisper")
        if sherpa:
            runner.test_backend(sherpa, audio_file, reference_text, "Sherpa")

    runner.print_report()


if __name__ == "__main__":
    main()
```

Also create tests/__init__.py (empty file):
```python
"""Test suite for Transkribator."""
```
  </action>
  <verify>python -m pytest tests/test_backend_quality.py -v --collect-only</verify>
  <done>test_backend_quality.py exists with calculate_wer, calculate_cer, measure_rtf functions</done>
</task>

<task type="auto">
  <name>Create test fixtures directory and sample README</name>
  <files>tests/fixtures/audio_samples/README.md</files>
  <action>
Create test fixtures structure:

```bash
mkdir -p tests/fixtures/audio_samples
```

Create tests/fixtures/audio_samples/README.md:
```markdown
# Audio Test Samples

This directory contains audio files for ASR quality testing.

## File Format

Each audio file should have a corresponding `.txt` file with the same name:

```
test.wav          # Audio sample (16kHz, mono)
test.wav.txt      # Reference transcription (Russian)
```

## Adding Test Samples

1. Place audio file in this directory (WAV or MP3, 16kHz mono recommended)
2. Create matching `.txt` file with exact transcription
3. Run: `python tests/test_backend_quality.py`

## Recommended Samples

- Short phrase (5-10 seconds): Quick iteration test
- Medium sentence (15-30 seconds): Real-world usage
- Long text (60+ seconds): Stability test
- Noisy audio: VAD effectiveness test
- Quiet audio: Hallucination detection test

## Example

```
poem.wav          # Recording of Russian poem
poem.wav.txt      # Exact text: "И тропинка, и лесок, В поле – каждый колосок!"
```
```
  </action>
  <verify>ls -la tests/fixtures/audio_samples/</verify>
  <done>fixtures directory exists with README.md explaining test format</done>
</task>

</tasks>

<verification>
After implementation:
1. test_backend_quality.py exists with all metric functions
2. Tests can be collected with pytest
3. Fixtures directory exists with README
4. Framework can be run directly: python tests/test_backend_quality.py
</verification>

<success_criteria>
- [ ] WER calculation implemented (Levenshtein on words)
- [ ] CER calculation implemented (Levenshtein on characters)
- [ ] RTF calculation implemented (processing_time / audio_duration)
- [ ] Test framework runs without errors
- [ ] Fixtures directory structured for adding samples
</success_criteria>

<output>
After completion, create .planning/phases/01-critical-bug-fixes/01-04-SUMMARY.md
</output>
