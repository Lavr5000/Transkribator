---
phase: 02-noise-reduction-vad
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - src/backends/sherpa_backend.py
  - RemotePackage/src/backends/sherpa_backend.py
  - src/config.py
autonomous: true

must_haves:
  truths:
    - "Silero VAD модель загружается через sherpa_onnx.OfflineVad"
    - "VAD детектирует сегменты речи в аудио (speech segments)"
    - "VAD параметры настраиваются: threshold, min_silence_duration_ms, min_speech_duration_ms"
    - "VAD модель скачивается автоматически если отсутствует"
  artifacts:
    - path: "src/backends/sherpa_backend.py"
      contains: "sherpa_onnx.OfflineVad"
      contains: "_vad"
      contains: "def _get_vad_model_dir"
    - path: "src/config.py"
      contains: "vad_enabled"
      contains: "vad_threshold"
      contains: "min_silence_duration_ms"
      contains: "min_speech_duration_ms"
  key_links:
    - from: "src/backends/sherpa_backend.__init__"
      to: "sherpa_onnx.OfflineVad"
      via: "lazy initialization in load_model"
      pattern: "OfflineVad("
    - from: "src/backends/sherpa_backend.transcribe"
      to: "VAD filtering"
      via: "speech segments extraction"
      pattern: "vad_stream\|segments"
---

<objective>
Integrate Silero VAD into SherpaBackend for speech detection before transcription.

**What:**
Add Silero VAD (Voice Activity Detection) to SherpaBackend using sherpa-onnx OfflineVad.

**Why:**
VAD removes silence from audio before ASR transcription, reducing processing time by 20-40% and improving WER by 5-15% (silence doesn't create false transcription artifacts). Current SherpaBackend has no VAD - processes entire audio including silence.

**Output:**
SherpaBackend with Silero VAD integration, configurable VAD parameters, automatic model download.
</objective>

<execution_context>
@C:\Users\user\.claude\get-shit-done\workflows\execute-plan.md
@C:\Users\user\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-noise-reduction-vad/02-RESEARCH.md
@.planning/phases/02-noise-reduction-vad/02-CONTEXT.md
@.planning/codebase/ARCHITECTURE.md

# Current file locations
C:\Users\user\.claude\0 ProEKTi\Transkribator\src\backends\sherpa_backend.py
C:\Users\user\.claude\0 ProEKTi\Transkribator\RemotePackage\src\backends\sherpa_backend.py
C:\Users\user\.claude\0 ProEKTi\Transkribator\src\config.py
</context>

<tasks>

<task type="auto">
  <name>Add VAD configuration parameters</name>
  <files>src/config.py</files>
  <action>
Add VAD settings to Config dataclass after line 24 (after enable_post_processing):

```python
# VAD (Voice Activity Detection) settings
vad_enabled: bool = True  # Enable VAD to remove silence before transcription
vad_threshold: float = 0.5  # Speech probability threshold (0.0-1.0)
min_silence_duration_ms: int = 800  # Min silence to mark speech end (milliseconds)
min_speech_duration_ms: int = 500  # Min speech to start detection (milliseconds)
```

These values match CONTEXT.md decisions:
- threshold=0.5: Moderate sensitivity
- min_silence=800ms: Medium patience (allows thinking pauses)
- min_speech=500ms: Only full phrases (rejects brief noise)
  </action>
  <verify>grep -n "vad_enabled\|vad_threshold\|min_silence_duration_ms\|min_speech_duration_ms" src/config.py</verify>
  <done>Config has all VAD parameters with CONTEXT.md values</done>
</task>

<task type="auto">
  <name>Add VAD initialization to SherpaBackend</name>
  <files>src/backends/sherpa_backend.py, RemotePackage/src/backends/sherpa_backend.py</files>
  <action>
In both sherpa_backend.py files:

1. Add imports at top (after existing sherpa_onnx import):
```python
from pathlib import Path
```

2. Add VAD attributes to __init__ after existing attributes (find self._recognizer = None around line 30-40):
```python
# VAD (Voice Activity Detection)
self._vad = None
self._vad_enabled = True  # Will be set from config
self._vad_threshold = 0.5
self._min_silence_duration_ms = 800
self._min_speech_duration_ms = 500
```

3. Add method to get VAD model directory (after _check_model_files method):
```python
def _get_vad_model_dir(self) -> Path:
    """Get Silero VAD model directory, download if missing."""
    from huggingface_hub import snapshot_download

    vad_dir = Path(__file__).parent.parent.parent / "models" / "sherpa" / "silero-vad"
    vad_dir.mkdir(parents=True, exist_ok=True)

    # Check if model exists, download if missing
    if not (vad_dir / "v4.onnx").exists():
        try:
            snapshot_download(
                repo_id="csukuangfj/sherpa-onnx-silero-vad",
                local_dir=str(vad_dir),
                local_dir_use_symlinks=False,
            )
        except Exception as e:
            print(f"Failed to download VAD model: {e}")
            # Return directory anyway - OfflineVad will fail gracefully

    return vad_dir
```

4. Update load_model() to initialize VAD (after recognizer creation, around line 180):
```python
# Initialize Silero VAD if enabled
self._vad = None
if self._vad_enabled:
    try:
        vad_dir = self._get_vad_model_dir()
        self._vad = sherpa_onnx.OfflineVad(
            model_dir=str(vad_dir),
            threshold=self._vad_threshold,
            min_silence_duration=self._min_silence_duration_ms / 1000.0,  # Convert to seconds
            min_speech_duration=self._min_speech_duration_ms / 1000.0,
        )
        print(f"VAD initialized: threshold={self._vad_threshold}")
    except Exception as e:
        print(f"Failed to initialize VAD: {e}")
        self._vad = None
```
  </action>
  <verify>grep -n "_get_vad_model_dir\|OfflineVad\|_vad_enabled" src/backends/sherpa_backend.py RemotePackage/src/backends/sherpa_backend.py</verify>
  <done>SherpaBackend has VAD initialization with automatic model download</done>
</task>

<task type="auto">
  <name>Add VAD speech filtering to transcribe method</name>
  <files>src/backends/sherpa_backend.py, RemotePackage/src/backends/sherpa_backend.py</files>
  <action>
In both sherpa_backend.py files, modify transcribe() method to apply VAD filtering:

Find the existing transcribe() method (around line 200-250). The method currently:
1. Calls load_model()
2. Creates stream and accepts waveform
3. Decodes and returns text

INSERT VAD filtering between load_model() and stream creation:

```python
def transcribe(self, audio: np.ndarray, sample_rate: int = 16000) -> Tuple[str, float]:
    """Transcribe audio with optional VAD pre-processing."""
    if self._recognizer is None:
        self.load_model()

    start_time = time.time()

    try:
        # Apply VAD to filter silence if enabled
        if self._vad_enabled and self._vad is not None:
            vad_stream = self._vad.create_stream()
            vad_stream.accept_waveform(sample_rate, audio)

            # Flush to process remaining audio
            self._vad.compute(vad_stream)

            # Get speech segments
            segments = vad_stream.segments

            if segments:
                # Concatenate only speech segments (remove silence)
                speech_segments = []
                for seg in segments:
                    start_sample = int(seg.start * sample_rate)
                    end_sample = int(seg.end * sample_rate)
                    # Ensure indices are within bounds
                    start_sample = max(0, start_sample)
                    end_sample = min(len(audio), end_sample)
                    if end_sample > start_sample:
                        speech_segments.append(audio[start_sample:end_sample])

                if speech_segments:
                    audio = np.concatenate(speech_segments)
                else:
                    # No speech detected
                    return "", 0.0
            else:
                # No speech segments detected
                return "", 0.0

        # Rest of existing transcribe code continues...
        stream = self._recognizer.create_stream()
        stream.accept_waveform(16000, audio)
        # ... rest unchanged
```

DO NOT modify the existing stream creation, decoding, or text extraction logic.
  </action>
  <verify>grep -n "vad_stream\|vad.create_stream\|vad.compute\|speech_segments" src/backends/sherpa_backend.py RemotePackage/src/backends/sherpa_backend.py</verify>
  <done>transcribe() applies VAD filtering before ASR, returns empty string if no speech</done>
</task>

</tasks>

<verification>
After implementation:
1. grep confirms vad_enabled, vad_threshold, min_silence_duration_ms in config
2. grep confirms _get_vad_model_dir method in sherpa_backend
3. grep confirms sherpa_onnx.OfflineVad initialization in load_model
4. grep confirms VAD filtering in transcribe method (vad_stream, segments, speech_segments)
5. Changes synchronized between local and RemotePackage versions
</verification>

<success_criteria>
- [ ] Config has VAD parameters matching CONTEXT.md decisions
- [ ] _get_vad_model_dir downloads Silero VAD model if missing
- [ ] OfflineVad initialized with threshold, min_silence, min_speech from config
- [ ] transcribe() filters silence before ASR using VAD segments
- [ ] Returns empty string if no speech detected
- [ ] Both local and RemotePackage versions updated
</success_criteria>

<output>
After completion, create `.planning/phases/02-noise-reduction-vad/02-03-SUMMARY.md`
</output>
